{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mirko Michele D'Angelo - Assignment 3\n",
    "\n",
    "First we load the data of the MNIST dataset, both training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import idx2numpy\n",
    "#load training images\n",
    "tr_images=idx2numpy.convert_from_file('./dataset/train-images.idx3-ubyte')\n",
    "tr_labels=idx2numpy.convert_from_file('./dataset/train-labels.idx1-ubyte')\n",
    "#load test images\n",
    "ts_images=idx2numpy.convert_from_file('./dataset/t10k-images.idx3-ubyte')\n",
    "ts_labels=idx2numpy.convert_from_file('./dataset/t10k-labels.idx1-ubyte')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "First we implement our RBM along the CD-1 training algorithm:\n",
    "- The __ __init__ __ constructor wll just initialize the values used for the biases and weights, in particulare the weights are initialized according to ... with a gaussian distribution $N(0,0.01)$ the same goes for the hidden biases.\n",
    "- __sample_hidden__ and __sample_visible__ implement for the corrisponding operation of sampling $h$ given $v$ and sampling $v$ given $h$.\n",
    "- the __train__ method implements the actual training using the CD-1 algorithm with minibatch and MSE as a loss function to monitor the reconstruction error.\n",
    "- the __encode__ method allows us to get the hidden activations and use them to encode data\n",
    "- also a simple sigmoid implementation and a utility sampling method are used to implement the other methods in the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM:\n",
    "    def __init__(self,visible_size,hidden_size):\n",
    "\n",
    "        self.visible_bias= np.zeros(visible_size,dtype='float64')\n",
    "        self.hidden_bias= np.zeros(hidden_size,dtype='float64')\n",
    "\n",
    "        self.weights=np.random.normal(scale=0.01,size=(visible_size,hidden_size))\n",
    "        print(f\"buildinig a RBM with {visible_size} visible units and {hidden_size} hidden units\")\n",
    "    def _sigmoid(self,x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    def _sample(self,prob):\n",
    "        return (prob > np.random.rand(*prob.shape)).astype(np.float64)\n",
    "    def sample_hidden(self,v):\n",
    "        ha_prob= self._sigmoid(v@self.weights+self.hidden_bias)\n",
    "        ha_states= self._sample(ha_prob)\n",
    "        return ha_prob,ha_states\n",
    "    def sample_visible(self,h):\n",
    "        recon_prob= self._sigmoid(h@self.weights.T+self.visible_bias)\n",
    "        recon_act= self._sample(recon_prob)\n",
    "        return recon_prob,recon_act\n",
    "\n",
    "    def train(self,values,eta=0.01,epochs=100,batch_size=64):\n",
    "        print(f\"training over {values.shape[0]} samples with {values.shape[1]} features \\nepochs={epochs}\\t batch size={batch_size}\\t learning rate={eta}\")\n",
    "        for e in range(epochs):\n",
    "            for i in range(0,values.shape[0],batch_size):\n",
    "                # clamp data as input\n",
    "                #clamped_data= self._sample(values[i:i+batch_size])\n",
    "                clamped_data= values[i:i+batch_size]\n",
    "                #sample h given v\n",
    "                ha_prob,ha_states=self.sample_hidden(clamped_data)\n",
    "                #calculate wake part\n",
    "                wake=clamped_data.T@ha_prob\n",
    "                #sample v given h\n",
    "                recon_prob,recon_act=self.sample_visible(ha_states)\n",
    "                active_prob=self._sigmoid(recon_act@self.weights+ self.hidden_bias)\n",
    "                #calculate dream part\n",
    "                dream=recon_act.T@active_prob\n",
    "                delta_w=(wake-dream)/batch_size\n",
    "                delta_bh = (np.mean(ha_prob-active_prob, axis=0))\n",
    "                delta_bv = (np.mean(clamped_data-recon_act, axis=0))\n",
    "\n",
    "                self.weights+=eta*delta_w\n",
    "                self.hidden_bias+=eta*delta_bh\n",
    "                self.visible_bias+=eta*delta_bv\n",
    "            clamped_data= self._sample(values)\n",
    "            ha_prob,ha_states=self.sample_hidden(clamped_data)\n",
    "            recon_prob,recon_act=self.sample_visible(ha_states)\n",
    "            print(f\"epoch no.{e+1} reconstruction error: {np.mean((clamped_data-recon_act)**2)}\")\n",
    "    def encode(self,data):\n",
    "        #sample h given v\n",
    "        _,ha_states=self.sample_hidden(data)\n",
    "        print(f\"{ha_states.shape[0]} samples encoded with {ha_states.shape[1]} hidden units\")\n",
    "        return ha_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBM training\n",
    "Now we train the RBM, first the data from mnist dataset is flattened from a $(28 \\times 28)$ matrix of integers between 0 and 255 to an array of 768 integers.\n",
    "After the flattening binarization is applied with a threshold of 127 to get values that are either 0 or 1.\n",
    "\n",
    "The reason for the flattening is just to be able to feed it into the rbm while the binarization is useful since it allows the contrastive divergence algorithm to properly work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_and_binarize(images,threshold=127):\n",
    "    return (images.reshape((-1,28*28))>threshold).astype(np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buildinig a RBM with 784 visible units and 50 hidden units\n",
      "training over 60000 samples with 784 features \n",
      "epochs=10\t batch size=64\t learning rate=0.2\n",
      "epoch no.1 reconstruction error: 0.09091751700680271\n",
      "epoch no.2 reconstruction error: 0.08384447278911565\n",
      "epoch no.3 reconstruction error: 0.08105393282312925\n",
      "epoch no.4 reconstruction error: 0.07953365221088435\n",
      "epoch no.5 reconstruction error: 0.07842778486394558\n",
      "epoch no.6 reconstruction error: 0.07743263180272109\n",
      "epoch no.7 reconstruction error: 0.07674034863945578\n",
      "epoch no.8 reconstruction error: 0.07583216411564626\n",
      "epoch no.9 reconstruction error: 0.07530625\n",
      "epoch no.10 reconstruction error: 0.07453975340136054\n"
     ]
    }
   ],
   "source": [
    "rbm=RBM(28*28,50)\n",
    "\n",
    "training=flatten_and_binarize(tr_images)\n",
    "rbm.train(training,\n",
    "          eta=0.2,\n",
    "          epochs=10,\n",
    "          batch_size=64\n",
    "          )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same treatment also goes for the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=flatten_and_binarize(ts_images)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting binary activations\n",
    "Using the encode method i can encode both training and test set of images using the hidden states activations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## classifier performances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 samples encoded with 50 hidden units\n",
      "10000 samples encoded with 50 hidden units\n"
     ]
    }
   ],
   "source": [
    "h_train=rbm.encode(training)\n",
    "h_test=rbm.encode(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mirda\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 955    0    0    2    0   10    5    6    2    0]\n",
      " [   0 1110    4    1    0    4    4    0   12    0]\n",
      " [   7    9  919   18   11    2   14    8   41    3]\n",
      " [   4    1   16  922    2   25    2   12   18    8]\n",
      " [   1    1    7    2  904    1   13    4   10   39]\n",
      " [  14    3    5   38   14  761   10    7   32    8]\n",
      " [   9    4    6    0    7   20  905    2    5    0]\n",
      " [   2    9   23    9    6    2    0  938    3   36]\n",
      " [   9   12    6   27    9   32   10   10  848   11]\n",
      " [   8    7    1   13   32    7    1   26    6  908]]\n",
      "raw 0.917\n",
      "[[ 933    0   11    0    2   11    9    0   13    1]\n",
      " [   0 1099    7    5    1    3    5    1   14    0]\n",
      " [  16    3  899   24   15    4   19   13   35    4]\n",
      " [   5    1   30  868    0   45    4   13   31   13]\n",
      " [   1    1   13    2  851    6   16    7   16   69]\n",
      " [  13    6    7   64   16  720   17    6   34    9]\n",
      " [  19    3    9    3   13   14  886    1    9    1]\n",
      " [   2    9   28    5   18    3    1  904    5   53]\n",
      " [  16    7    8   26   13   27   12    4  839   22]\n",
      " [   7    7    3   11   66   12    2   41   15  845]]\n",
      "encoded 0.8844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mirda\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "#raw training with values\n",
    "mlp=LogisticRegression(max_iter=100).fit(training,tr_labels)\n",
    "pred=mlp.predict(test)\n",
    "print(confusion_matrix(ts_labels,pred))\n",
    "print(f\"raw {accuracy_score(ts_labels,pred)}\")\n",
    "# training on encoded values\n",
    "mlp=LogisticRegression(max_iter=100).fit(h_train,tr_labels)\n",
    "pred=mlp.predict(h_test)\n",
    "print(confusion_matrix(ts_labels,pred))\n",
    "print(f\"encoded {accuracy_score(ts_labels,pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mirda\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mirda\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_base.py:1237: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "C:\\Users\\mirda\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\sklearn\\svm\\_classes.py:31: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 957    0    1    1    1    8    6    3    3    0]\n",
      " [   0 1109    5    1    0    3    4    0   13    0]\n",
      " [  13    9  900   19   11    3   17   11   45    4]\n",
      " [   6    2   21  909    4   25    5   14   17    7]\n",
      " [   2    2    4    2  910    2   10    3    7   40]\n",
      " [  11    3    3   38   12  765   15    8   29    8]\n",
      " [  12    4    4    3    6   20  905    0    4    0]\n",
      " [   2   13   24    7    6    4    0  939    1   32]\n",
      " [  10   14   10   21   13   32   13   12  835   14]\n",
      " [   7    8    0   12   42   10    1   32   16  881]]\n",
      "raw 0.911\n",
      "[[ 939    0    8    4    3    4    9    0   12    1]\n",
      " [   0 1109    7    3    0    1    3    0   12    0]\n",
      " [  21    5  882   22   14    6   18   21   39    4]\n",
      " [   9    1   36  853    0   42    4   17   31   17]\n",
      " [   3    2   14    3  834    6   14    6   16   84]\n",
      " [  19   10    9   78   18  683   18    9   38   10]\n",
      " [  22    4    7    2   10   13  886    2   10    2]\n",
      " [   2   13   28    2   20    4    1  897    7   54]\n",
      " [  16    9    8   25   14   22   13    5  846   16]\n",
      " [  10   11    8   15   67   10    2   37   12  837]]\n",
      "encoded 0.8766\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "#raw training with values\n",
    "mlp=LinearSVC().fit(training,tr_labels)\n",
    "pred=mlp.predict(test)\n",
    "print(confusion_matrix(ts_labels,pred))\n",
    "print(f\"raw {accuracy_score(ts_labels,pred)}\")\n",
    "# training on encoded values\n",
    "mlp=LinearSVC().fit(h_train,tr_labels)\n",
    "pred=mlp.predict(h_test)\n",
    "print(confusion_matrix(ts_labels,pred))\n",
    "print(f\"encoded {accuracy_score(ts_labels,pred)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
