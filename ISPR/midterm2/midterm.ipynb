{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mirko Michele D'Angelo - Assignment 3\n",
    "\n",
    "First we load the data of the MNIST dataset, both training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import idx2numpy\n",
    "#load training images\n",
    "tr_images=idx2numpy.convert_from_file('./dataset/train-images.idx3-ubyte')\n",
    "tr_labels=idx2numpy.convert_from_file('./dataset/train-labels.idx1-ubyte')\n",
    "#load test images\n",
    "ts_images=idx2numpy.convert_from_file('./dataset/t10k-images.idx3-ubyte')\n",
    "ts_labels=idx2numpy.convert_from_file('./dataset/t10k-labels.idx1-ubyte')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "First we implement our RBM with the CD-1 training algorithm, the implementation is in the RBM class with the following methods:\n",
    "- The __ __init__ __ constructor wll just initialize the values used for the biases and weights.\n",
    "- __sample_hidden__ implement for the corrisponding operation of sampling hidden units from the visible ones.\n",
    "- __sample_visible__ implement for the corrisponding operation of sampling hidden visible from the hidden ones.\n",
    "- the __train__ method implements the actual training using the CD-1 algorithm with minibatch and MSE to monitor the reconstruction error.\n",
    "- the __encode__ method allows us to get the activations of hidden units and use them to encode data.\n",
    "- the __reconstruct__ method allows us to reconstsruct data.\n",
    "\n",
    "Also a simple sigmoid implementation and a utility sampling method are used to implement the other methods in the class.\n",
    "## parameters initialization\n",
    "Parameters are initialized according to [this paper](http://www.cs.toronto.edu/%7Ehinton/absps/guideTR.pdf) :\n",
    "\n",
    "Weights are initialized with a gaussian distribution $N(0,0.01)$, the hidden biases with a value of 0. The visible biases are all initialized with value $\\log(p_i/(1-p_i))$ where $p_i$ is is the proportion of training vectors in which unit $i$ is on, also note that since $p_i$ might become positive infinite or negative infinite due to precision erros in the fraction the values that go to infinite are clipped to have the maximum or minimum finite value available depending on their sign. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBM:\n",
    "    def __init__(self,visible_size,hidden_size):\n",
    "        self.visible_bias= np.zeros(visible_size,dtype='longdouble')\n",
    "        self.hidden_bias= np.zeros(hidden_size,dtype='longdouble')\n",
    "\n",
    "        self.weights=np.random.normal(scale=0.01,size=(visible_size,hidden_size))\n",
    "        print(f\"buildinig a RBM with {visible_size} visible units and {hidden_size} hidden units\")\n",
    "    def _sigmoid(self,x):\n",
    "        return 1/(1+np.exp(-x))\n",
    "    def _sample(self,prob):\n",
    "        return (prob > np.random.rand(*prob.shape)).astype(np.longdouble)\n",
    "    def sample_hidden(self,v):\n",
    "        ha_prob= self._sigmoid(v@self.weights+self.hidden_bias)\n",
    "        ha_states= self._sample(ha_prob)\n",
    "        return ha_prob,ha_states\n",
    "    def sample_visible(self,h):\n",
    "        recon_prob= self._sigmoid(h@self.weights.T+self.visible_bias)\n",
    "        recon_act= self._sample(recon_prob)\n",
    "        return recon_prob,recon_act\n",
    "\n",
    "    def train(self,values,eta=0.01,epochs=100,batch_size=64):\n",
    "        print(f\"training over {values.shape[0]} samples with {values.shape[1]} features \\nepochs={epochs}\\t batch size={batch_size}\\t learning rate={eta}\")\n",
    "        for e in range(epochs):\n",
    "            for i in range(0,values.shape[0],batch_size):\n",
    "                # clamp data as input\n",
    "                clamped_data= values[i:i+batch_size]\n",
    "                #sample h given v\n",
    "                ha_prob,ha_states=self.sample_hidden(clamped_data)\n",
    "                #calculate wake part\n",
    "                wake=clamped_data.T@ha_prob\n",
    "                #sample v given h\n",
    "                recon_prob,recon_act=self.sample_visible(ha_states)\n",
    "                active_prob=self._sigmoid(recon_act@self.weights+ self.hidden_bias)\n",
    "                #calculate dream part\n",
    "                dream=recon_act.T@active_prob\n",
    "                delta_w=(wake-dream)/batch_size\n",
    "                delta_bh = (np.mean(ha_prob-active_prob, axis=0))\n",
    "                delta_bv = (np.mean(clamped_data-recon_act, axis=0))\n",
    "\n",
    "                self.weights+=eta*delta_w\n",
    "                self.hidden_bias+=eta*delta_bh\n",
    "                self.visible_bias+=eta*delta_bv\n",
    "            clamped_data= values\n",
    "            recon_act= self.reconstruct(clamped_data)\n",
    "            print(f\"epoch no.{e+1} reconstruction error: {np.mean((clamped_data-recon_act)**2)}\")\n",
    "    def reconstruct(self,data):\n",
    "        _,ha_states=self.sample_hidden(data)\n",
    "        recon_prob,recon_act=self.sample_visible(ha_states)\n",
    "        return recon_act\n",
    "    def encode(self,data):\n",
    "        #sample h given v\n",
    "        _,ha_states=self.sample_hidden(data)\n",
    "        print(f\"{ha_states.shape[0]} samples encoded with {ha_states.shape[1]} hidden units\")\n",
    "        return ha_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RBM training\n",
    "Now we train the RBM, first the data from mnist dataset is flattened from a $(28 \\times 28)$ matrix of integers between 0 and 255 to an array of 768 integers.\n",
    "After the flattening binarization is applied with a random threshold generated by a uniform distribution. The reason for the flattening is just to be able to feed it into the rbm while the binarization is useful since it allows the contrastive divergence algorithm to properly work.\n",
    "\n",
    "The training is done with a small grid search over hidden units and learning rates since changing batch sizes, at least during my tests, doesn't affect a lot the final results. The dataset has been split using a holdout with 80% training set a 20% validation set, the best  model on the validation set is chosen as the final RBM to encode values in the dataset as retrained on the whole training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_and_binarize(images):\n",
    "    threshold=np.random.rand(*images.shape)\n",
    "    return (images>threshold).reshape((-1,28*28)).astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buildinig a RBM with 784 visible units and 50 hidden units\n",
      "{'hidden_units': 50, 'learning_rate': 0.001}\n",
      "buildinig a RBM with 784 visible units and 50 hidden units\n",
      "training over 48000 samples with 784 features \n",
      "epochs=10\t batch size=10\t learning rate=0.001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mirda\\AppData\\Local\\Temp\\ipykernel_22312\\3213545003.py:25: RuntimeWarning: divide by zero encountered in log\n",
      "  rbm.visible_bias=np.log(visible_active_prob/(1-visible_active_prob))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 reconstruction error: 0.19294637542517007\n",
      "epoch no.2 reconstruction error: 0.16376270195578232\n",
      "epoch no.3 reconstruction error: 0.14713847257653062\n",
      "epoch no.4 reconstruction error: 0.13762178465136055\n",
      "epoch no.5 reconstruction error: 0.13135615965136055\n",
      "epoch no.6 reconstruction error: 0.1265911724064626\n",
      "epoch no.7 reconstruction error: 0.12242421343537414\n",
      "epoch no.8 reconstruction error: 0.11903063881802721\n",
      "epoch no.9 reconstruction error: 0.11609271364795919\n",
      "epoch no.10 reconstruction error: 0.11349782100340136\n",
      "validation set reconstruction error 0.11312127976190477\n",
      "{'hidden_units': 50, 'learning_rate': 0.01}\n",
      "buildinig a RBM with 784 visible units and 50 hidden units\n",
      "training over 48000 samples with 784 features \n",
      "epochs=10\t batch size=10\t learning rate=0.01\n",
      "epoch no.1 reconstruction error: 0.113707137542517\n",
      "epoch no.2 reconstruction error: 0.09929474914965987\n",
      "epoch no.3 reconstruction error: 0.09257315582482993\n",
      "epoch no.4 reconstruction error: 0.08861176658163265\n",
      "epoch no.5 reconstruction error: 0.0858546981292517\n",
      "epoch no.6 reconstruction error: 0.08418635735544218\n",
      "epoch no.7 reconstruction error: 0.08287327274659864\n",
      "epoch no.8 reconstruction error: 0.08165837585034014\n",
      "epoch no.9 reconstruction error: 0.08084584927721089\n",
      "epoch no.10 reconstruction error: 0.0800930325255102\n",
      "validation set reconstruction error 0.07969504676870748\n",
      "{'hidden_units': 50, 'learning_rate': 0.1}\n",
      "buildinig a RBM with 784 visible units and 50 hidden units\n",
      "training over 48000 samples with 784 features \n",
      "epochs=10\t batch size=10\t learning rate=0.1\n",
      "epoch no.1 reconstruction error: 0.08247031781462585\n",
      "epoch no.2 reconstruction error: 0.07924255952380953\n",
      "epoch no.3 reconstruction error: 0.07756816007653061\n",
      "epoch no.4 reconstruction error: 0.0767366868622449\n",
      "epoch no.5 reconstruction error: 0.0761796875\n",
      "epoch no.6 reconstruction error: 0.07572480867346938\n",
      "epoch no.7 reconstruction error: 0.07516929740646258\n",
      "epoch no.8 reconstruction error: 0.07469480761054421\n",
      "epoch no.9 reconstruction error: 0.07455115327380953\n",
      "epoch no.10 reconstruction error: 0.07410615965136054\n",
      "validation set reconstruction error 0.07380792942176871\n",
      "{'hidden_units': 100, 'learning_rate': 0.001}\n",
      "buildinig a RBM with 784 visible units and 100 hidden units\n",
      "training over 48000 samples with 784 features \n",
      "epochs=10\t batch size=10\t learning rate=0.001\n",
      "epoch no.1 reconstruction error: 0.1679029017857143\n",
      "epoch no.2 reconstruction error: 0.1393288424744898\n",
      "epoch no.3 reconstruction error: 0.12705835459183673\n",
      "epoch no.4 reconstruction error: 0.11885015412414966\n",
      "epoch no.5 reconstruction error: 0.11262816220238095\n",
      "epoch no.6 reconstruction error: 0.10750738732993197\n",
      "epoch no.7 reconstruction error: 0.10348073448129251\n",
      "epoch no.8 reconstruction error: 0.10005776998299319\n",
      "epoch no.9 reconstruction error: 0.09705054209183674\n",
      "epoch no.10 reconstruction error: 0.0944804156037415\n",
      "validation set reconstruction error 0.09411670918367347\n",
      "{'hidden_units': 100, 'learning_rate': 0.01}\n",
      "buildinig a RBM with 784 visible units and 100 hidden units\n",
      "training over 48000 samples with 784 features \n",
      "epochs=10\t batch size=10\t learning rate=0.01\n",
      "epoch no.1 reconstruction error: 0.09510828550170068\n",
      "epoch no.2 reconstruction error: 0.08067392113095238\n",
      "epoch no.3 reconstruction error: 0.07414891581632653\n",
      "epoch no.4 reconstruction error: 0.07013703762755102\n",
      "epoch no.5 reconstruction error: 0.06731746917517006\n",
      "epoch no.6 reconstruction error: 0.06534491921768708\n",
      "epoch no.7 reconstruction error: 0.06380824829931973\n",
      "epoch no.8 reconstruction error: 0.06258564519557823\n",
      "epoch no.9 reconstruction error: 0.061509964923469385\n",
      "epoch no.10 reconstruction error: 0.06059608843537415\n",
      "validation set reconstruction error 0.06042123724489796\n",
      "{'hidden_units': 100, 'learning_rate': 0.1}\n",
      "buildinig a RBM with 784 visible units and 100 hidden units\n",
      "training over 48000 samples with 784 features \n",
      "epochs=10\t batch size=10\t learning rate=0.1\n",
      "epoch no.1 reconstruction error: 0.0645304262329932\n",
      "epoch no.2 reconstruction error: 0.05927784863945578\n",
      "epoch no.3 reconstruction error: 0.0573523331207483\n",
      "epoch no.4 reconstruction error: 0.056164992559523806\n",
      "epoch no.5 reconstruction error: 0.05464317602040816\n",
      "epoch no.6 reconstruction error: 0.053754703443877554\n",
      "epoch no.7 reconstruction error: 0.05260990646258503\n",
      "epoch no.8 reconstruction error: 0.05216908482142857\n",
      "epoch no.9 reconstruction error: 0.051624627976190475\n",
      "epoch no.10 reconstruction error: 0.05089676339285714\n",
      "validation set reconstruction error 0.05104039115646258\n",
      "{'hidden_units': 200, 'learning_rate': 0.001}\n",
      "buildinig a RBM with 784 visible units and 200 hidden units\n",
      "training over 48000 samples with 784 features \n",
      "epochs=10\t batch size=10\t learning rate=0.001\n",
      "epoch no.1 reconstruction error: 0.1481733630952381\n",
      "epoch no.2 reconstruction error: 0.12440029761904763\n",
      "epoch no.3 reconstruction error: 0.11192926232993197\n",
      "epoch no.4 reconstruction error: 0.10371540178571428\n",
      "epoch no.5 reconstruction error: 0.0974483949829932\n",
      "epoch no.6 reconstruction error: 0.09253377444727891\n",
      "epoch no.7 reconstruction error: 0.08862686011904762\n",
      "epoch no.8 reconstruction error: 0.08525884885204081\n",
      "epoch no.9 reconstruction error: 0.08251698022959184\n",
      "epoch no.10 reconstruction error: 0.08017458545918367\n",
      "validation set reconstruction error 0.07979251700680272\n",
      "{'hidden_units': 200, 'learning_rate': 0.01}\n",
      "buildinig a RBM with 784 visible units and 200 hidden units\n",
      "training over 48000 samples with 784 features \n",
      "epochs=10\t batch size=10\t learning rate=0.01\n",
      "epoch no.1 reconstruction error: 0.08085302402210884\n",
      "epoch no.2 reconstruction error: 0.06676496067176871\n",
      "epoch no.3 reconstruction error: 0.060156808035714285\n",
      "epoch no.4 reconstruction error: 0.05612917198129252\n",
      "epoch no.5 reconstruction error: 0.053391687925170066\n",
      "epoch no.6 reconstruction error: 0.051256776147959184\n",
      "epoch no.7 reconstruction error: 0.049646285076530614\n",
      "epoch no.8 reconstruction error: 0.048179474914965985\n",
      "epoch no.9 reconstruction error: 0.04717718962585034\n",
      "epoch no.10 reconstruction error: 0.04628818558673469\n",
      "validation set reconstruction error 0.04631855867346939\n",
      "{'hidden_units': 200, 'learning_rate': 0.1}\n",
      "buildinig a RBM with 784 visible units and 200 hidden units\n",
      "training over 48000 samples with 784 features \n",
      "epochs=10\t batch size=10\t learning rate=0.1\n",
      "epoch no.1 reconstruction error: 0.05226987670068027\n",
      "epoch no.2 reconstruction error: 0.04645538371598639\n",
      "epoch no.3 reconstruction error: 0.043424266581632655\n",
      "epoch no.4 reconstruction error: 0.04178231292517007\n",
      "epoch no.5 reconstruction error: 0.04061779868197279\n",
      "epoch no.6 reconstruction error: 0.03956986075680272\n",
      "epoch no.7 reconstruction error: 0.03894406356292517\n",
      "epoch no.8 reconstruction error: 0.03835613307823129\n",
      "epoch no.9 reconstruction error: 0.037548788265306124\n",
      "epoch no.10 reconstruction error: 0.03713047406462585\n",
      "validation set reconstruction error 0.03756887755102041\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid, train_test_split\n",
    "\n",
    "random_state=42\n",
    "\n",
    "np.random.seed(random_state)\n",
    "\n",
    "rbm=RBM(28*28,50)\n",
    "\n",
    "grid=ParameterGrid({\n",
    "    \"hidden_units\":[50,100,200],\n",
    "    \"learning_rate\":[0.001,0.01,0.1],\n",
    "})\n",
    "\n",
    "training=flatten_and_binarize(tr_images)\n",
    "X_train,X_test,y_train,y_test=train_test_split(training,tr_labels,test_size=0.2,random_state=random_state,stratify=tr_labels)\n",
    "best_loss=+np.inf\n",
    "best_rbm=None\n",
    "for params in grid:\n",
    "    print(params)\n",
    "    rbm=RBM(28*28,params['hidden_units'])\n",
    "    #initialize visible bias with suggested values\n",
    "    visible_active_prob=np.mean(X_train,axis=0)\n",
    "    maxv=max(rbm.visible_bias[np.isfinite(rbm.visible_bias)])\n",
    "    minv=min(rbm.visible_bias[np.isfinite(rbm.visible_bias)])\n",
    "    rbm.visible_bias=np.log(visible_active_prob/(1-visible_active_prob))\n",
    "    rbm.visible_bias=np.clip(rbm.visible_bias,minv,maxv)\n",
    "    \n",
    "    rbm.train(X_train,\n",
    "            eta=params['learning_rate'],\n",
    "            epochs=10,\n",
    "            batch_size=10\n",
    "            )\n",
    "    recon_act=rbm.reconstruct(X_test)\n",
    "    loss=np.mean((X_test-recon_act)**2)\n",
    "    print(f\"validation set reconstruction error {loss}\")\n",
    "    if loss < best_loss:\n",
    "        best_loss=loss\n",
    "        best_params=params\n",
    "        best_rbm=rbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final retraining on the whole training set for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buildinig a RBM with 784 visible units and 200 hidden units\n",
      "training over 60000 samples with 784 features \n",
      "epochs=10\t batch size=10\t learning rate=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mirda\\AppData\\Local\\Temp\\ipykernel_22312\\665517387.py:7: RuntimeWarning: divide by zero encountered in log\n",
      "  final_rbm.visible_bias=np.log(visible_active_prob/(1-visible_active_prob))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch no.1 reconstruction error: 0.050544727891156466\n",
      "epoch no.2 reconstruction error: 0.044902763605442174\n",
      "epoch no.3 reconstruction error: 0.04290257227891157\n",
      "epoch no.4 reconstruction error: 0.04124880952380952\n",
      "epoch no.5 reconstruction error: 0.03955297619047619\n",
      "epoch no.6 reconstruction error: 0.038997108843537416\n",
      "epoch no.7 reconstruction error: 0.037762393707482994\n",
      "epoch no.8 reconstruction error: 0.03736887755102041\n",
      "epoch no.9 reconstruction error: 0.037178890306122446\n",
      "epoch no.10 reconstruction error: 0.036231016156462584\n",
      "test reconstruction error 0.03673545918367347\n"
     ]
    }
   ],
   "source": [
    "test=flatten_and_binarize(ts_images)\n",
    "final_rbm=RBM(28*28,best_params['hidden_units'])\n",
    "#intialize visible bias with suggested values\n",
    "visible_active_prob=np.mean(X_train,axis=0)\n",
    "maxv=max(final_rbm.visible_bias[np.isfinite(final_rbm.visible_bias)])\n",
    "minv=min(final_rbm.visible_bias[np.isfinite(final_rbm.visible_bias)])\n",
    "final_rbm.visible_bias=np.log(visible_active_prob/(1-visible_active_prob))\n",
    "final_rbm.visible_bias=np.clip(final_rbm.visible_bias,minv,maxv)\n",
    "\n",
    "final_rbm.train(training,eta=best_params['learning_rate'],epochs=10,batch_size=10)\n",
    "\n",
    "test_rec=final_rbm.reconstruct(test)\n",
    "print(f\"test reconstruction error {np.mean((test-test_rec)**2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## encoding data\n",
    "Now we first binarize the test data and then encode both training and test data using the hidden activations of the RBM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 samples encoded with 200 hidden units\n",
      "10000 samples encoded with 200 hidden units\n"
     ]
    }
   ],
   "source": [
    "\n",
    "h_train=final_rbm.encode(training)\n",
    "h_test=final_rbm.encode(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# using the encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the encoding of both training and test set to train a logistic regression to recognize the MNIST digits.\n",
    "The logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Ignore convergence warnings\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.93125 with parameters: {'C': 0.1, 'max_iter': 100, 'penalty': 'l2'}\n",
      "Accuracy: 0.93125 with parameters: {'C': 0.1, 'max_iter': 200, 'penalty': 'l2'}\n",
      "Accuracy: 0.93125 with parameters: {'C': 0.1, 'max_iter': 300, 'penalty': 'l2'}\n",
      "Accuracy: 0.9315833333333333 with parameters: {'C': 1, 'max_iter': 100, 'penalty': 'l2'}\n",
      "Accuracy: 0.9315833333333333 with parameters: {'C': 1, 'max_iter': 200, 'penalty': 'l2'}\n",
      "Accuracy: 0.9315833333333333 with parameters: {'C': 1, 'max_iter': 300, 'penalty': 'l2'}\n",
      "Accuracy: 0.9304166666666667 with parameters: {'C': 10, 'max_iter': 100, 'penalty': 'l2'}\n",
      "Accuracy: 0.93075 with parameters: {'C': 10, 'max_iter': 200, 'penalty': 'l2'}\n",
      "Accuracy: 0.93075 with parameters: {'C': 10, 'max_iter': 300, 'penalty': 'l2'}\n",
      "best parameters {'C': 1, 'max_iter': 100, 'penalty': 'l2'}\n",
      "f1-score on test set: 0.9365\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import  train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#make a model selection of the logistic regression\n",
    "# Split the data into training and testing sets\n",
    "X_train,X_test,y_train,y_test=train_test_split(h_train,tr_labels,test_size=0.2,random_state=random_state,stratify=tr_labels)\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {'C': [0.1, 1, 10], 'penalty': ['l2'],'max_iter':[100,200,300]}\n",
    "#param grid fire the linear SVM\n",
    "best_fscore=-np.inf\n",
    "best_params=None\n",
    "for params in ParameterGrid(param_grid):\n",
    "    model = LogisticRegression(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    fscore = f1_score(y_test, y_pred,average='micro')\n",
    "    if fscore > best_fscore:\n",
    "        best_fscore = fscore\n",
    "        best_params = params\n",
    "    print(f\"Accuracy: {fscore} with parameters: {params}\")\n",
    "# Train the best model\n",
    "print(f\"best parameters {best_params}\")\n",
    "model = LogisticRegression(**best_params)\n",
    "model.fit(h_train, tr_labels)\n",
    "# Evaluate the model\n",
    "y_pred = model.predict(h_test)\n",
    "print(f\"f1-score on test set: {f1_score(ts_labels, y_pred,average='micro')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results and final considerations\n",
    "\n",
    "In the first part the RBM, selected through a model selection, manages to achieve 0.03 of reconstrunction error on the test set showing a small loss of information during the reconstruction using 200 hidden units a batch size of 10 during training and a learning rate of 0.1. \n",
    "For the second part the logistic regression selected on a model selection using th same split used for the selected RBM manages to get a 0.93 of f1-score using microaveraging.\n",
    "\n",
    "RBMs are good model that have the capability of learning representations from data, they offer a nice dimensionality reduction that lets use simpler models to achieve good results. However RBMs can be costly to train when we have a lot of data and we use a lot of hidden activations the trainig time increases significantly, also they offer little interpretation for the learned features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
